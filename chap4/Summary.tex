The application works as such that the first screen the user sees when launching the application, on both Google Glass and smartphones, is the camera screen. The user is then to hposition the device in such a was that the QR code may be scanned by the device's camera. The QR code contains a product ID for a specific product.

The user does not need to press any shutter button in order to scan the QR code. Instead the application will automatically recognise any QR code pattern that appears in camera view, as well as scan the QR code. The reason for not implementing a start menu or any similar start screen, to show the user before the camera screen, is to, according to Google's design guidelines, keep the focus on what the application is intended to do and to keep the application simple and easy to use.

Next the application will decode the QR code. The decoding process is done by the ZXing library, which is an open source  barcode image processing library. The QR code contains a product ID which is then used in the downloading process. The downloading process entails connecting to a database containing information on different products, and, by using the decoded product ID, retrieving the information on the specific product. 

The downloaded information contains the product name, as well as a list of components and the instructions necessary to construct the product. All the information is then sorted in to respective classes and the information may be displayed to the user. When the product information is being downloaded a loading animation is displayed on screen. On Google Glass the loading information is a loading bar at the bottom of the screen, and in the smartphone application the loading animation is a spinning wheel.

When the download process has finished the information is displayed to the user in the form of a slide show. The first slide that is displayed to the user is the title slide. The title slide contains the name of the product as well as an image  (if an image existed in the database). Following the title slide are the component slides. Each component has their own slide due to the fact that a component may be described in both text and an image. 

After all the component slides comes the instruction slides. Similar to the components an instruction my be presented by text only or by both text and an image. In contrast to the components, however, instructions may also be presented with an image and no text. 

As Google provides developers of Google Glass applications with predefined layouts, these layouts were also used for the Google Glass application. The layouts used were ``Title'', ``Columns'', ``Text'' and ``Caption''. The predefined layouts were also used as basis for the layouts used in the smartphone application.

The Title layout is used for the title card. The Columns layout is used for the slides with both text and an image. The Text layout is used for the slides with only text. The Caption layout is used for the slides with only an image. All layouts, except for the Title layout, also contains text at the bottom of the screen called ``footer'' and ``timestamp''. The footer contains information on wether the current slide is a component slide or an instruction slide. The timestamp contains information on which slide is currently being viewed.

While browsing through the slides in the Google Glass application, the user may also navigate using voice commands. The voice commands available in the Google Glass application are ``Show next slide'', ``Show previous slide'', ``Show components'', ``Show instructions'' and ``Scan again''.

Most of the voice commands follows 11 out of the 15 voice command guidelines provided by Google. For instance does ``Show components'' not follow the guidelines which states that ``Is general enough to apply to multiple Glassware, but still has a clear purpose''. ``Show components'' is a specific voice command and could potentially apply to multiple Google Glass applications, but not most.

While viewing the slides ``ok glass'' is shown at the bottom of the screen. ``ok glass'' indicates that voice commands are available and saying ``ok glass'' at that point brings up the voice command menu, showing all available voice commands. However, ``ok glass'' is also shown in combination with a dark, transparent overlay, which does ensure ``ok glass'' is always visible no matter what image is shown on screen, but the dark overlay does also mean that any image shown is darken by the overlay.

In terms of the testing done on the application the experimental setup consisted of an optical bench on which the QR code were positioned at the zero mark, and then each specific device were position as such that the camera of each device were positioned according to the specifications of each test. Each test result was then obtained through a laptop with which each device was connected via a USB cable. The test results was printed out from a timer class, called \texttt{Timer}, which was implemented using the singleton design pattern, meaning that the class only had one global instance which could be accessed from anywhere within the appllication.

The test which did not require the experimental setup was the text length test. Instead the text length test was performed using a class which randomised english characters, which were then added together to a large string.

Instead three other tests were done while using the experimental setup. In these tests the distance to the QR code, the complexity of the QR code and the size of the downloaded information respectively. Each of these tests were done 30 times, for each device and each different specification, to ensure statistical significance.