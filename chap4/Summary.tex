The application works in such a way that the first screen the user sees when launching the application, on both Google Glass and smartphones, is the camera screen. The user is then to position the device in such a way that the QR code may be scanned by the device's camera. The QR code contains a product ID for a specific product.

The user does not need to press any shutter button in order to scan the QR code. Instead the application will automatically recognise any QR code pattern that appears in the camera view, as well as scan the QR code. The reason for not implementing a start menu or any similar start screen, to show the user before the camera screen is displayed, is to, according to Google's design guidelines, maintain the focus on what the application is intended to do and to keep the application simple and easy to use.

Next the application will decode the QR code. The decoding process is done by the ZXing library, which is an open source  barcode image processing library. The QR code contains a product ID which is then used in the downloading process. The downloading process entails connecting to a database containing information about different products, and, by using the decoded product ID, retrieving the information on the specific product. 

The downloaded information contains the product name, as well as a list of components and the instructions necessary to assemble the product. All the information is then sorted in to respective classes and the information may be displayed to the user. When the product information is being downloaded a loading animation is displayed on screen. On Google Glass the loading information is a loading bar at the bottom of the screen, and in the smartphone application the loading animation is a spinning wheel.

When the download process has finished the information is displayed to the user in the form of a slide show. The first slide that is displayed to the user is the title slide. The title slide contains the name of the product as well as an image  (if an image existed in the database). Following the title slide are the component slides. Each component has their own slide due to the fact that a component may be described in both text and an image. 

After all the component slides follow the instruction slides. Similar to the components an instruction my be presented by text only or by both text and an image. In contrast to the components, however, instructions may also be presented with an image and no text. 

As Google provides developers of Google Glass applications with predefined layouts, these layouts were also used for the Google Glass application. The layouts used were ``Title'', ``Columns'', ``Text'' and ``Caption''. The predefined layouts were also used as basis for the layouts used in the smartphone application.

The Title layout is used for the title card. The Columns layout is used for the slides with both text and an image. The Text layout is used for the slides with text only. The Caption layout is used for the slides containing only an image. All layouts, except for the Title layout, also contain text at the bottom of the screen called ``footer'' and ``timestamp''. The footer contains information on whether the current slide is a component slide or an instruction slide. The timestamp contains information on which slide is currently being viewed.

While browsing through the slides in the Google Glass application, the user may also navigate using voice commands. The voice commands available in the Google Glass application are ``Show next slide'', ``Show previous slide'', ``Show components'', ``Show instructions'' and ``Scan again''.

Most of the voice commands follow 11 out of the 15 voice command guidelines provided by Google. For example, ``Show components'' does not follow the guidelines which state that ``Is general enough to apply to multiple Glassware, but still has a clear purpose''. ``Show components'' is a specific voice command and could potentially apply to multiple Google Glass applications, but not all.

While viewing the slides ``ok glass'' is shown at the bottom of the screen. ``ok glass'' indicates that voice commands are available and saying ``ok glass'' at that point brings up the voice command menu, showing all available voice commands. However, ``ok glass'' is also shown in combination with a dark, transparent overlay, which ensures ``ok glass'' is always visible no matter what image is shown on screen, but the dark overlay also means that any image shown is darkened by the overlay.

In terms of the testing performed on the application, the experimental setup consisted of an optical bench on which the QR code was positioned at the zero mark, and then each specific device was positioned so that the camera of each device was positioned according to the specifications of each test. Each test result was then obtained through a laptop with which each device was connected via a USB cable. The test results were printed out from a timer class, called \texttt{Timer}, which was implemented using the singleton design pattern, meaning that the class had only one global instance which could be accessed from anywhere within the application.

The test which did not require the experimental setup was the text length test. Instead the text length test was performed using a class which randomised English characters, which were then concatenated together to form a longer string.

Three other tests were performed using the experimental setup. In the first of these three tests the distance to the QR code was varied. In the second test the complexity of the QR code was varied. In the third and final test the size of the downloaded information was varied. Each of these tests were performed 30 times, for each device and each different specification, to ensure statistical significance.